{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: Logistic Regression\n",
    "\n",
    "###  Step 1: Function set\n",
    "\n",
    "上节课推导了概率模型$\\mathrm{p}_{w,b}(C_1 | x) = \\sigma(z), \\sigma(z) = \\frac{1}{1+\\exp (-z)}$,其中$z = w ·x+b=\\sum_i{w_i x_i + b}$\n",
    "\n",
    "![](./ml_png/sigmoidFunction.png)\n",
    "\n",
    "得到了概率模型后，进行目标的分类，其中\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\mathrm{p}_{w,b}(C_1 | x) \\geq 0.5, & class \\ 1 \\\\\n",
    "\\mathrm{p}_{w,b}(C_1 | x) \\lt  0.5 , & class \\ 2 \\\\\n",
    "\\end{cases}  \\tag{1.0}$$\n",
    "\n",
    "结合sigmoid function的函数特性，亦即\n",
    "\n",
    "$$\\begin{cases}\n",
    "z \\geq 0, & class \\ 1 \\\\\n",
    "z \\lt  0 , & class \\ 2 \\\\\n",
    "\\end{cases}  \\tag{1.1}$$\n",
    "\n",
    "针对上面的$z = w ·x+b=\\sum_i{w_i x_i + b}$,可以绘制出下面的图形，\n",
    "\n",
    "![](./ml_png/3_function_set1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Goodness of a Function [Loss function]\n",
    "\n",
    "Training Data: \n",
    "\n",
    "![](./ml_png/3_training_data0.png)\n",
    "\n",
    "这些训练数据是怎么得来的呢？假设它们是根据$f_{w,b}(x) = \\mathrm{p}_{w,b}(C_1 |x) = \\mathrm{p}_{\\mu,\\Sigma}(C_1 |x)$这个概率模型采样得来。但是$w,b$的选择是任意的，即不同的概率模型$(\\mu,\\Sigma)$下都有可能获得这些训练数据，区别只在于同时获得这些数据的可能性大小。如果要将这种可能性最大化，用数学公式来表达就是，\n",
    "\n",
    "$$  \n",
    "L(w,b) = f_{w,b}\\left( x^1 \\right) · f_{w,b}\\left( x^2 \\right) · \\left(1-f_{w,b}\\left( x^3 \\right)\\right) · \\dots ·  f_{w,b}\\left( x^N \\right) \n",
    " \\tag{1.2}$$\n",
    "\n",
    "进一步地采用maximum likehood来进行最大化，求出$w^*, b^*$即可，于是有\n",
    "\n",
    "$$ \\begin{align}\n",
    "w^*, b^* &= \\arg \\max_{w,b} L(w,b) \\\\\n",
    "&= \\arg \\min_{w,b} \\Bigl \\{ -\\ln L(w,b) \\Bigr\\}\n",
    "\\end{align}  \\tag{1.3} $$\n",
    "\n",
    "对$-\\ln L(w,b) $, 有\n",
    "\n",
    "$$\\begin{align}\n",
    "-\\ln L(w,b) = &-\\ln f_{w,b}\\left( x^1 \\right)  \\\\\n",
    "&- \\ln f_{w,b}\\left( x^2 \\right)    \\\\\n",
    "&- \\ln \\left(1-f_{w,b}\\left( x^3 \\right)\\right) \\\\\n",
    "&- \\dots  \\\\\n",
    "&- \\ln f_{w,b}\\left( x^N \\right) \\label{123} \\tag{1.4} \n",
    "\\end{align}$$\n",
    "\n",
    "将class 1和class 2的值分别置为1和0（现在为二分类），训练数据输出值为具体的数值\n",
    "\n",
    "![](./ml_png/3_training_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了对公式$(1.4)$进行简化，变成累加的形式，结合$y^n$的具体取值，我们可以得到\n",
    "\n",
    "$$\\begin{align}\n",
    "-\\ln L(w,b) &= \\left[ \\hat{y}^1\\ln f\\left( x^1 \\right) + \\left( 1- \\hat{y}^1 \\right) \\ln \\left(1-f\\left( x^1 \\right) \\right) \\right]  \\\\\n",
    "& \\ \\ \\ -\\left[ \\hat{y}^2\\ln f\\left( x^2 \\right) + \\left( 1- \\hat{y}^2 \\right) \\ln \\left(1-f\\left( x^2 \\right) \\right) \\right]  \\\\\n",
    "& \\ \\ \\ -\\left[ \\hat{y}^3\\ln f\\left( x^3 \\right) + \\left( 1- \\hat{y}^3 \\right) \\ln \\left(1-f\\left( x^3 \\right) \\right) \\right]  \\\\\n",
    "& \\ \\ \\ - \\dots  \\\\\n",
    "& \\ \\ \\ -\\left[ \\hat{y}^N\\ln f\\left( x^N \\right) + \\left( 1- \\hat{y}^N \\right) \\ln \\left(1-f\\left( x^N \\right) \\right) \\right]  \\\\\n",
    "&= \\sum_{n=1}^N{-\\left[ \\hat{y}^n\\ln f\\left( x^n \\right) + \\left( 1- \\hat{y}^n \\right) \\ln \\left(1-f\\left( x^n \\right) \\right) \\right]}\n",
    "\\label{15} \\tag{1.5} \n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有Bernoulli Distribution p:\n",
    "\n",
    "\\begin{cases}\n",
    "\\mathrm{p} (x=1) = \\hat{y}^n \\\\\n",
    "\\mathrm{p} (x=0) = 1-\\hat{y}^n \n",
    "\\tag{1.6} \\end{cases}\n",
    "\n",
    "同时有Bernoulli Distribution q:\n",
    "\n",
    "\\begin{cases}\n",
    "\\mathrm{q} (x=1) = f(x^n) \\\\\n",
    "\\mathrm{q} (x=0) = 1-f(x^n)\n",
    "\\tag{1.7} \\end{cases}\n",
    "\n",
    "那么它们之间的交叉熵（cross entropy）为：\n",
    "\n",
    "$$\n",
    "H(\\mathrm{p},\\mathrm{q})=  -\\sum_x \\mathrm{p}(x) \\ln\\left( \\mathrm{q}(x) \\right)\n",
    "\\tag{1.8} $$\n",
    "\n",
    "采用one-hot的形式来对物体的真实类别进行管理，则相当于training data符合Bernoulli分布，那么可以认为**公式$(1.5)$也是两个Bernoulli分布之间的cross entropy.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Find the best function \n",
    "\n",
    "目前已经完成了对模型的构造$f_{w,b} = \\mathrm{p}(C_1 | x)$，同时也已经定义损失函数$-\\ln L(w,b)$，接下来要考虑的是对参数的更新，以找到最佳的映射函数。参数更新的常用方法是Gradient Descent，因此先对参数求微分，即\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\Bigl\\{-\\ln L(w,b)\\Bigr\\}}{\\partial w_i} \n",
    "&= \\frac{\\partial \\sum_{n=1}^N{-\\left[ \\hat{y}^n\\ln f\\left( x^n \\right) + \\left( 1- \\hat{y}^n \\right) \\ln \\left(1-f\\left( x^n \\right) \\right) \\right]} }{\\partial w_i}  \\\\\n",
    "&= \\sum_{n=1}^N{-\\left[ \\hat{y}^n  \\frac{\\partial  \\ln f\\left( x^n \\right)}{\\partial w_i}  + \\left( 1- \\hat{y}^n \\right) \\frac{\\partial  \\ln \\left(1-f\\left( x^n \\right) \\right)}{\\partial w_i} \\right]}  \\\\\n",
    "\\tag{1.9} \\end{align}$$\n",
    "\n",
    "其中$f(x_i^n) = \\sigma(w·x_i^n+b)$，则\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial  \\ln f\\left( x^n \\right)}{\\partial w_i} &= \\frac{1}{f(x_i^n)} · \\frac{\\partial f(x_i^n)}{\\partial w_i}  \\\\\n",
    "&= \\frac 1\\sigma ·\\sigma (1-\\sigma)· x_i^n  \\\\\n",
    "&= (1-\\sigma)· x_i^n  \\\\\n",
    "&= \\left(1-f(x_i^n) \\right)· x_i^n  \\\\\n",
    "\\tag{1.10} \\end{align}$$\n",
    "\n",
    "同理，\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial  \\ln \\left( 1- f\\left( x^n \\right) \\right) }{\\partial w_i} \n",
    "&= \\frac{1}{(1-\\sigma)}·\\left[ -\\sigma (1-\\sigma) \\right]· x_i^n  \\\\\n",
    "&= -\\sigma· x_i^n  \\\\\n",
    "&= -f(x_i^n)·x_i^n \\\\\n",
    "\\tag{1.11} \\end{align}$$\n",
    "\n",
    "因此\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\Bigl\\{-\\ln L(w,b)\\Bigr\\}}{\\partial w_i} \n",
    "&= \\sum_{i=1}^{N}{-\\left[ \\hat{y}^n  \\left( 1-f(x_i^n)  \\right) · x_i^n - (1-\\hat{y}^n) f(x_i^n)·x_i^n \\right]} \\\\\n",
    "&= -\\sum_{i=1}^{N}{\\left[ \\hat{y}^n - f_{w,b}(x^n) \\right] x_i^n} \\\\\n",
    "\\tag{1.12} \\end{align}$$\n",
    "\n",
    "求得gradient后，使用梯度下降法进行参数更新，有\n",
    "$$\n",
    "w_i \\leftarrow w_i + \\eta \\Sigma_n \\left[ \\hat{y}^n - f_{w,b}(x^n) \\right] x_i^n\n",
    "\\tag{1.13} $$\n",
    "\n",
    "不断循环前向传播和反向传播，直至迭代次数上限或收敛趋于平稳。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**讨论一**\n",
    "\n",
    "上述步骤是logistics regression + cross entropy的组合完成classification。$w,b$值的计算都是通过gradient descent来完成求解，而非如上节所述的先假设分布函数，通过中间值$\\mu^1, \\mu^2, \\Sigma$再一步到位求解得到$w,b$。其中的loss function的选取也是遵照了maximum likelihood的原则推导出来。既然第一节中loss function的选取是根据square error来选取的，如果选用它替代cross entropy会有怎样的效果呢？\n",
    "\n",
    "如下图。\n",
    "\n",
    "![](./ml_png/3_loregre_squareError_1.png)\n",
    "\n",
    "![](./ml_png/3_loregre_squareError_3.png)\n",
    "\n",
    "根据计算得到的gradient可发现，由于梯度同时存在$f_{w,b}(x)$和$\\left( 1-f_{w,b}(x) \\right)$项，则它们的梯度值更新量总是趋于零。正常情况下，我们认为梯度值趋于零的地方为局部（或全局）最小值（或saddle point）。但在使用square error作为loss function进行classification时，当我们初始化一个$w_i$时，第一次计算所得的$\\partial w_i$就已经很小，一次选择就已经到接近极小值点，这显然是不合理的。因此，这种情况是整个loss function都很平坦，不利于参数的更新。相反，使用cross entropy作为loss function则避免了这种情况。如$(1.13)$，当真实值$\\hat{y}^n$与计算值$f_{w,b}(x)$的距离很大时，这种关系会反应到梯度值上面。\n",
    "\n",
    "![](./ml_png/3_cross_squareError.png)\n",
    "[Source](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**讨论二**\n",
    "\n",
    "Logistic Regression和Linear regression之间的对比。不同：function set和loss function；相同：参数更新策略。\n",
    "\n",
    "![](./ml_png/3_l_l_regression_compare.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**从讨论部分回到classification。**上节课讲了使用概率的方法先计算$\\mu^1,\\mu^2,\\Sigma$,后代入$w,b$来计算某目标属于某类的概率$\\mathrm{p}(C_1|x)$。这节课的内容则直接计算$w,b$.两方法是否能得到一致的分类准确度呢？答案是否定的。先分别赋给这两种方法一个名字，上节课所用方法是生成式方法，本节课所使用的是判别式方法。**则其中的一个原因是**,生成式方法事先规定了所采集数据的分布规律，但这些人为假定的分布不一定是它们的真实分布。用一个例子来讲解。\n",
    "\n",
    "![](./ml_png/3_l_l_regression_compare_example2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有如上的训练数据（$x_1=1,x_2=1$为第一类，$x_1=1,x_2=0$、$x_1=0,x_2=1$和$x_1=0,x_2=0$为第二类）和测试数据（测试数据为$x_1=1,x_2=1$）。根据生成式的方法，需计算以下几个概率，\n",
    "$$\\mathrm{p}(C_1) =\\frac{1}{13}, \\mathrm{p}(C_2) =\\frac{12}{13}, \\mathrm{p}(x_1=1 | C_1)=1, \\mathrm{p}(x_2 = 1 | C_1)=1, \\mathrm{p}(x_1=1 | C_2)=\\frac13, \\mathrm{p}(x_2 = 1 | C_2)=\\frac13, $$\n",
    "\n",
    "进一步地，\n",
    "$$\n",
    "\\mathrm{p}(x|C_1) = \\mathrm{p}(x_1=1|C_1)·\\mathrm{p}(x_2=1|C_1) = 1 \\times 1 = 1 \\\\\n",
    "\\mathrm{p}(x|C_2) = \\mathrm{p}(x_1=1|C_2)·\\mathrm{p}(x_2=1|C_2) = \\frac 13 \\times \\frac 13 = \\frac 19\n",
    "$$\n",
    "\n",
    "根据贝叶斯公式有，\n",
    "$$\\begin{align}\n",
    "\\mathrm{p}(C_1|x) &= \\frac{\\mathrm{p}(x|C_1) \\mathrm{p}(C_1)}{\\mathrm{p}(x|C_1) \\mathrm{p}(C_1) + \\mathrm{p}(x|C_2) \\mathrm{p}(C_2)} \\\\\n",
    "&= \\frac{1· \\frac{1}{13}}{1· \\frac{1}{13} + \\frac 19· \\frac{12}{13}}  \\\\\n",
    "&= \\frac 37 \\lt 0.5\n",
    "\\end{align}$$\n",
    "\n",
    "根据生成式进行结果归类，测试数据为$x_1=1,x_2=1$的目标被归类为class 2.但class1中有与测试数据一致的目标，正确的归类结果应该是class 1才对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
