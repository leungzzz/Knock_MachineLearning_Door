{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for DL\n",
    "\n",
    "Deep learning的常规流程\n",
    "\n",
    "![](./ml_png/6_normal_dl_process.png)\n",
    "\n",
    "一些对应的改进措施：\n",
    "\n",
    "①，在Training data上表现差两个应对方案：\n",
    "\n",
    "    A. New Activation Function\n",
    "    B. Adaptive learning rate\n",
    "    \n",
    "②，在Testing data上表现出的三个应对方案：\n",
    "\n",
    "    A. Early stopping\n",
    "    B. Regularization\n",
    "    c. Dropout\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New activation function\n",
    "\n",
    "问题：为什么要用新的？原来的sigmoid activation function出了什么问题？有哪些新的activation function?\n",
    "\n",
    "回答：随着层数的不断加深，旧的sigmoid function 会引起Vanishing gradient problem.考虑梯度值\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial z}{\\partial w_i}· \\frac{\\partial L}{\\partial z} = \n",
    " \\frac{\\partial z}{\\partial w_i} \\left(  \\frac{\\partial a}{\\partial z}· \\frac{\\partial L}{\\partial a}  \\right) = \n",
    " x_i · \\sigma'(z)· \\left[  w_4 ·  \\frac{\\partial L}{\\partial z'}  +  w_5· \\frac{\\partial L}{\\partial z''}  \\right]\n",
    "$$\n",
    "\n",
    "其中$\\sigma'(z) = \\sigma(z) \\left(  1-\\sigma(z)  \\right), \\ \\ 0<\\sigma(z) < 1$. 如果$z'$和$z''$后面还有上百层的$z^{i}$,那么整个由多个$\\sigma$构成的数值将会变得非常小，如$0.99^{100} \\approx 0.3660$, 当基底变小，有$0.9^{100} \\approx 2.656\\times 10^{-5}$.此时整个gradient会变得非常小。进行gradient descent时，有\n",
    " \n",
    "$$\n",
    "w_1 = w_0 - \\eta \\frac{\\partial L}{\\partial w_0}\n",
    "$$\n",
    "\n",
    "其中$w_0$为初始的随机化weight参数。在反向传播时，越靠近output的gradient由于相乘的$\\sigma$较少，故$\\frac{\\partial L}{\\partial w_N} $较大，学习较快，收敛的速度也会更快。而随着backward pass的不断进行，越靠近Input的gradient,如$w_1$越小，参数学习的速度也相对更慢，可能而$w_1$还在缓慢地向极值靠近，$w_N$就已经根据前面的$w$值收敛到某个极值附近。考虑到所有weight参数都是随机初始的，前面的weight值随机且更新很慢，而后面的weight值已经收敛。极端情况下，靠近input的weight值在随机初始化的初始值附近缓慢更新，而靠近output的weight已经停止更新。此时总的参数更新已经相当缓慢，收敛到某loss值附近，会被误认为已经到达极值附近并发出停止training的错误信号.\n",
    "\n",
    "正是由于这种梯度缓慢消失的情况出现，因此sigmoid被认为不合理，矛盾点在于激活函数的多次相乘所得到的值趋向于零。\n",
    "\n",
    "另一种说法，为什么sigmoid function有问题的解释是，如下图。\n",
    "\n",
    "![](./ml_png/6_relu_reason.png)\n",
    "\n",
    "求偏微分的过程可以简单地理解为自变量的变化（$\\Delta w$）对因变量值的变化（$\\Delta l$）大小，直觉上我们期望这个值大些。因为它反映了这个model的灵敏程度。比如将model用于猫狗分类时，如果loss对输入的dog,cat图片的响应相差不大，那么一定程度上代表这个model很难对不同的class进行分类。而应用sigmoid function时，正是这种情况，即loss对weight的变化不敏感。从上右图sigmoid function两组input值和两组Ouput值可反应出该问题。尤其是当layer不断加深而这种效应的不断叠加，loss前和loss后的值将相差不大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "为了缓解梯度消失的问题，第一个解决方案是activation function更改为ReLU。\n",
    "\n",
    "![](./ml_png/6_relu.png)\n",
    "\n",
    "ReLU的特性。在输出的$z = \\sigma(w_i x_i + b) <0$ 时，$a=0$; 而$z>0$时，$a=z, a'=1$.此时，从$\\frac{\\partial L}{\\partial w_i}$公式的角度便不再出现多个$a'=\\sigma'(z)$的相乘会导致gradient消失的情况。因为$a$值不再在$0<a<1$的范围。\n",
    "\n",
    "如果从Neural Network Structure的角度来看。则可能存在如下可能性。\n",
    "\n",
    "![](./ml_png/6_relu2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果Layer1中的$z_1^3,z_1^4<0$，则$a_1^3,a_1^4=0$,相当于这个neuron被抛弃，同理Layer2中$z$值小于零时有同样的效果。最终去除了$z_1^3,z_1^4,z_2^1, z_2^3$所在的neurons的NN如上图右图示。该过程与overfitting过程的dropout类似（dropout需要定义dropout rate，这里不需要，因此灵活度更大）。分析最终形成的thinner linear network，其中的neurons大幅度减少，总体参数量减少。靠近input的weight的smaller gradients的情况得到缓解，因为全连接权重被压缩为原来的一半，计算gradient需要累乘的$\\sigma'(a)$个数减少为原来的一半。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU的变体有以下两种，它们都是针对$z<0$的情况而言，如下图\n",
    "\n",
    "![](./ml_png/6_leakyrelu_prelu.png)\n",
    "\n",
    "其中Leaky ReLU在$a$轴左侧有斜率$\\alpha=0.01$，parametric ReLU中左侧的斜率值$\\alpha$是学习得到的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了ReLU以外，还有一种激活函数也是常用的,即Maxout。该激活函数的动态的，被称为learnable activation function.这里的maxout就是输出包含了多个$z$的one group中的最大值。如下图\n",
    "\n",
    "![](./ml_png/6_maxout.png)\n",
    "\n",
    "这里的一个neuron包含了$z=\\sum_i w_i x_i +b$和$a=max\\{ z_i^1,z_i^2 \\}$两个变量的求值。Maxout可视为ReLU的更一般形式，或者说，ReLU是Maxout的特殊形式。如下图\n",
    "\n",
    "![](./ml_png/6_maxout_relu.png)\n",
    "\n",
    "左侧是对$z=w x +b$进行ReLU运算，可画出左下角的折线图，其中$z$小于零的地方置零；右侧是比较出直线$z_1=w x +b$和直线$z_2=0 \\times x +0 \\times 1$之间的最大值$a= \\max{z_1, z_2}$，然后取他们的最大值作为输出。需注意的是，Maxout还可以表示除ReLU以外的其他激活函数。分别用$w',b'$来取代上面右侧图的$0,0$,则分别有直线$z_1 = wx+b$和$z_2=w'x+b'$，如下图\n",
    "\n",
    "![](./ml_png/6_maxout_morethan_relu.png)\n",
    "\n",
    "总结：\n",
    "\n",
    "1. 在Maxout Network中，该Maxout function可以是任意的分段线性凸函数；\n",
    "\n",
    "2. “分段”中的段数取决于一组中有多少个$z$值。上面的几幅图中$z$值为2，所有activation function被分为了两段。除此以外，还有可能分三段及以上，如下图所示\n",
    "\n",
    "    ![](./ml_png/6_maxout_group.png)\n",
    "\n",
    "3. Maxout的训练过程分为三步，第一步是先找到每个neuron的最大值；第二步是去除非最大值所在的neuron；第三步是训练剩余的network,如下图。\n",
    "\n",
    "    ![](./ml_png/6_maxout_eventually2.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3_420]",
   "language": "python",
   "name": "conda-env-Anaconda3_420-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
