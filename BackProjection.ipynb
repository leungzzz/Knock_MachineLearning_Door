{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "首先，需要知道BP是干嘛的。BP是快速计算gradient的一种方法，在DNN中gradient主要是指对$\\frac {\\partial L}{\\partial W}$的高效计算。\n",
    "\n",
    "在gradient descent中，每次都需要对DNN中的参数$W,b$的梯度进行更新，其中网络参数有:\n",
    "\n",
    "$$\n",
    "\\theta = \\Bigl\\{  W_1, W_2, \\dots, b_1, b_2, \\dots,\\Bigr\\}\n",
    "\\tag{5.1}$$\n",
    "\n",
    "各网络参数相对于Loss的梯度为\n",
    "\n",
    "$$\n",
    "\\nabla L(\\theta) = \\left[ \\begin{array}{r} \\frac {\\partial L(\\theta)}{\\partial W_1}  \\\\\n",
    "\\frac {\\partial L(\\theta)}{\\partial W_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac {\\partial L(\\theta)}{\\partial b_1}  \\\\\n",
    "\\frac {\\partial L(\\theta)}{\\partial b_2}  \\\\\n",
    "\\vdots \\\\\n",
    "\\end{array}  \\right]\n",
    "\\tag{5.2}$$\n",
    "\n",
    "由于网络参数的迭代过程为：\n",
    "\n",
    "$$\n",
    "\\theta^0  \\longrightarrow \\theta^1 \\longrightarrow \\theta^2 \\longrightarrow \\dots \\longrightarrow \\theta^N\n",
    "\\tag{5.3}$$\n",
    "\n",
    "则每更新一次总参数值可表示为：\n",
    "\n",
    "$$\n",
    "\\mathrm{Compute}  \\  \\nabla L(\\theta^0), \\ \\theta^1 = \\theta^0 - \\eta \\nabla L(\\theta^0) \\\\\n",
    "\\mathrm{Compute}  \\  \\nabla L(\\theta^1), \\ \\theta^2 = \\theta^1 - \\eta \\nabla L(\\theta^1) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathrm{Compute}  \\  \\nabla L(\\theta^{N-1}), \\ \\theta^{N} = \\theta^{N-1} - \\eta \\nabla L(\\theta^{N-1}) \\\\\n",
    "$$\n",
    "\n",
    "在计算$\\frac{\\partial L} {\\partial w_1}$和$\\frac{\\partial L} {\\partial w_2}$时，如果$W_1$和$W_2$是从输入$x_1,x_2$到$Z_1=\\sum_i W_i x_i+b$的weight。那么在计算这两个gradient时会出现很多重复的计算。为了提高效率，减少不必要的计算量，backpropagation应运而生。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先需要了解的是两种不同形式的chain rule.\n",
    "\n",
    "<mark style=background-color:yellow>形式一（直链式）</mark>\n",
    "\n",
    "两个不同的函数用公式表示为\n",
    "\n",
    "$$\n",
    "y = g(x), z=h(y)\n",
    "\\tag{5.4}$$\n",
    "\n",
    "图表达形式为\n",
    "$$\n",
    "\\boxed{ \\Delta x  \\rightarrow \\Delta y \\rightarrow \\Delta z  }\n",
    "$$\n",
    "\n",
    "求函数$z$相对于$x$的微分。有\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} z}{\\mathrm{d}x} = \\frac{\\mathrm{d} y}{\\mathrm{d}x} · \\frac{\\mathrm{d} z}{\\mathrm{d}y} \n",
    "\\tag{5.5}$$\n",
    "\n",
    "\n",
    "<mark style=background-color:yellow>形式二（分叉式）</mark>\n",
    "\n",
    "函数表示为\n",
    "\n",
    "$$\n",
    "x = g(s), y=h(s),z=k(x,y)\n",
    "\\tag{5.6}$$\n",
    "\n",
    "图表达形式为\n",
    "\n",
    "$$\\boxed{  \\begin{matrix}\n",
    "& & \\Delta x & &  \\\\\n",
    "& \\nearrow & & \\searrow &  \\\\\n",
    "\\Delta s & & & & \\Delta z  \\\\\n",
    "& \\searrow & & \\nearrow  &  \\\\\n",
    "& & \\Delta y  & &  \\\\\n",
    "\\end{matrix} }$$\n",
    "\n",
    "求函数$z$相对于$s$的微分。有\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} z}{\\mathrm{d}s} = \\frac{\\mathrm{d} x}{\\mathrm{d}s} · \\frac{\\mathrm{d} z}{\\mathrm{d}x} + \n",
    "\\frac{\\mathrm{d} y}{\\mathrm{d}s} · \\frac{\\mathrm{d} z}{\\mathrm{d}y} \n",
    "\\tag{5.7}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于BP的核心在于计算梯度，且计算的梯度是相对于Loss而言，NN的结构如图所示，\n",
    "\n",
    "$$\n",
    "\\boxed{x^n} \\longrightarrow  \\boxed{NN \\\\ (\\theta)}  \\longrightarrow \\boxed{y^n} \\stackrel{l^n}{\\Longleftrightarrow} \\boxed{\\hat{y}^n}\n",
    "$$\n",
    "\n",
    "则由loss到loss对weight求偏微分的过程为\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum_{n=1}^{N}{l^n(\\theta)} , \\Longrightarrow    \\frac{\\partial L(\\theta)}{\\partial W}=  \\sum_{n=1}^{N}{\\frac{\\partial l^n(\\theta)}{\\partial W} }\n",
    "\\tag{5.8}$$\n",
    "\n",
    "如果只是看其中一个$w_i$的计算，则根据下面的流程图进行分析。\n",
    "\n",
    "![](./ml_png/5_bp_gradient.png)\n",
    "\n",
    "根据chain rule,其中要计算的是\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w_i} = \\frac{\\partial z}{\\partial w_i} · \\frac{\\partial l}{\\partial z} \n",
    "\\tag{5.9}$$\n",
    "\n",
    "该过程包含了两个微分值的计算。其中前向传播是对参数计算$\\frac{\\partial z}{\\partial w_i}$，反向传播是对激活函数$a$的输入值$z$计算偏微分，即$\\frac{\\partial L}{\\partial z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "①，前者的值为，$\\frac{\\partial z}{\\partial w_1}=x_1, \\frac{\\partial z}{\\partial w_2}=x_2$，亦即由weight所连接的input就是$z$对weight的偏微分，\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial w_i}=x_i\n",
    "\\tag{5.10}$$\n",
    "\n",
    "②，后者的值，需要进一步做chain rule计算，即$\\frac{\\partial L}{\\partial z} = \\frac{\\partial a}{\\partial z}·\\frac{\\partial L}{\\partial a}$\n",
    "\n",
    "其中$\\frac{\\partial a}{\\partial z} = \\sigma'(z)$,而对$\\frac{\\partial L}{\\partial a}$的计算形如分叉式的chain rule.即\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial L}{\\partial a} &= \\frac{\\partial z'}{\\partial a}·\\frac{\\partial L}{\\partial z'} + \\frac{\\partial z''}{\\partial a}·\\frac{\\partial L}{\\partial z''}  \\\\\n",
    "&= w_3 ·  \\frac{\\partial L}{\\partial z'}+ w_4·\\frac{\\partial L}{\\partial z''}\n",
    "\\end{align} \\tag{5.11}$$\n",
    "\n",
    "则对第二步，有\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial z} = \\sigma '(z) \\left[  w_3 ·  \\frac{\\partial L}{\\partial z'}+ w_4·\\frac{\\partial L}{\\partial z''}  \\right]\n",
    "\\tag{5.12}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./ml_png/5_backward_pass.png)\n",
    "\n",
    "\n",
    "对完整的$w_1$求微分，有\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial l}{\\partial w_1} \n",
    "&= \\frac{\\partial z}{\\partial w_1} ·\\frac{\\partial l}{\\partial z}  \\\\\n",
    "&= x_1 ·\\boxed{\\sigma '(z)} \\left[  w_3 ·  \\boxed{\\frac{\\partial L}{\\partial z'}}+ w_4·\\boxed{\\frac{\\partial L}{\\partial z''}}  \\right] \\\\\n",
    "\\end{align}  \\tag{5.13}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结以上，可发现在计算$\\frac{\\partial L}{\\partial w_i}$时，统共需要计算的只有$\\sigma'(z)$和$\\frac{\\partial L}{\\partial z^{(i)}}$，其中的$\\sigma'(z)=\\sigma(z) \\left( 1 -  \\sigma \\left(z\\right) \\right)$也可以事先计算好。\n",
    "\n",
    "到了这一步，那么就可以琢磨一下forward pass（前向）和backward pass（反向）这两个词的真正含义了。所谓前向，就是从前往后，而后向，自然就是从后往前。为了计算$w_1$，需要使用位于$w_1$前面的$x_1$；为了计算$\\frac{\\partial l}{\\partial z}$，需要使用$z$后面的$w_3, w_4$、计算上面的\\sigma导数和多个微分值$\\frac{\\partial l}{\\partial z'}$和$\\frac{\\partial l}{\\partial z''}$等。“前”与“后”都是相对于涉及到的(待计算的)变量值而言。下图是后向传播的参数计算。\n",
    "\n",
    "![](./ml_png/5_back_pass_back.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在$(5.13)$中，仍然有$\\frac{\\partial L}{\\partial z'}$需要计算。当前的计算又可分为两种情况:\n",
    "1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
