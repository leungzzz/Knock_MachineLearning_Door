{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 机器学习的概念：\n",
    "\n",
    "  机器学习就是自动找函数的过程。\n",
    "    \n",
    "   具体例子：\n",
    "   \n",
    "   Speech recognition\n",
    "   Image Recofnition\n",
    "   Playing Go(围棋)\n",
    "   对话系统\n",
    "   \n",
    "2. 具体要找怎样的函数？ 任务不同，要找的函数就不同，那么相对应的机器学习算法也不同。\n",
    "   其中最基本的两种机器学习方法是：回归和分类（分类分为binary classification和multi-class classification）\n",
    "   \n",
    "3. 怎样告诉机器你要找的函数是什么？\n",
    "   监督学习：给定标注的数据、给定loss、根据一定的策略找出令Loss最低的函数形式。\n",
    "   强化学习：\n",
    "   非监督学习（auto-encoder）：所给定的数据是无类别的标签。\n",
    "   \n",
    "4. 机器怎样找到你想要的的函数？\n",
    "   1）限制函数的寻找范围；Network Architecture \\ Use RNN \\ Use CNN\n",
    "   2) 函数的寻找方法：Gradient Descent\n",
    "      a. Inplement the algorithm by yourself\n",
    "      b. Deep Learning Framework\n",
    "\n",
    "5. 前言研究：\n",
    "   1） Explaining AI\n",
    "   2) Adversarial Attack: 增加噪声\n",
    "     比如说，原来是无噪声的，那么通过model识别出来的是cat.现在增加了噪声，识别为starfish，应该如何处理这种情况。\n",
    "   3） Network Compression: 将网络参数通过删改进行压缩，并尽可能保持不影响它的性能。\n",
    "   以上三种方法都依托于对CNN结果的认识及应用\n",
    "   \n",
    "   4）Anomaly Detection（异常检测）\n",
    "     向训练好的模型中输入未曾被学习过的目标，网络的输出应该对应哪些内容。\n",
    "   5）Transform Learning （Domain Adversial Learning）\n",
    "     如Testing data与训练的data分布不一致，导致精度大打折扣。手写汉字识别，training使用二值化图像，但测试使用彩色图像。\n",
    "   6）Meta Learning(元学习）\n",
    "     学习如何去学习。具体应用有：\n",
    "     a. Life-long Learning(终身学习），也称为Continuous Learning、Never Ending Learning、Incremental Learning\n",
    "     b. Reinforcement Learning\n",
    "     \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二节课：Regression Case Study\n",
    "\n",
    "什么是Regression? 输入是x, 输出是一个scalar值的一种模型\n",
    "\n",
    "举例：股票市场预测：输入是前一天的股市指标 $\\rightarrow$  后一天道琼指数\n",
    "     自动驾驶：输入是马路的路况  $\\rightarrow$   方向盘角度\n",
    "     推荐系统：输入是某个人、某件商品  $\\rightarrow$   购买的可能性\n",
    "贯穿本章的另一个例子：输入是宝可梦精灵进化前的各种属性（特征），输出是进化后的战斗力cp值。\n",
    "\n",
    "其中宝可梦有如下属性：\n",
    "\n",
    "cp: Combat power, 战斗力\n",
    "$x_w$: weight; $x_h$: height; \n",
    "$x_s$: speceies; $x_hp$: 生命力值;  \n",
    "\n",
    "用函数形式表示如下：\n",
    "\n",
    "$$f(宝可梦进化前的属性/特征) = 进化后的cp值$$简写为： $$f(x) = y$$\n",
    "\n",
    "具体的预测过程可分为以下三步:\n",
    "\n",
    "** Step1: (Function set) Model **\n",
    "\n",
    "这个Function sets在这里取linear regression就是 $y = b + w · x_{cp}$，其中$w, b$都是待学习的参数, 当用到宝可梦的例子时，仅考虑进化前的cp属性，那么可以是$f_1, f_2, f_3, \\cdots$， 当使用具体的值代入$w, b$时，更具体的表达形式为\n",
    "$$f_1: y = 10.0 + 9.0 · x_{cp}$$\n",
    "\n",
    "$$ f_2: y = 9.8 + 9.2 · x_{cp}$$\n",
    "\n",
    "$$ f_3: y = -0.8 - 1.2 · x_{cp}$$ （由于$x_{cp}$非负，$f_3$的情况可排除）\n",
    "\n",
    "以上的这个model是一个linear model，在考虑更多其他属性值的时候，它的更一般化的形式为：$y = b+\\sum w_i x_i$，其中$b$为bias,它能让$y$值在竖直方向作上下移动；$w_i$为weight,用于衡量属性$i$的重要程度。\n",
    "\n",
    "** Step 2: Goodness of function **\n",
    "\n",
    "    衡量Function set中选取的函数$f$的好坏（这个步骤需要收集Training data）,衡量的具体标准是loss function(由于衡量的对象是一个函数$f$,而它本身也是一个函数，因此它是function's function)\n",
    "    \n",
    "function input: $ x^1, x^2, x^3, ..., x^{10} $\n",
    "\n",
    "function output: $ \\hat{y}^1, \\hat{y}^2, \\hat{y}^3, ..., \\hat{y}^{10} $\n",
    "\n",
    "这里的衡量标准，使用函数$L(f)$来表示，而$f$本身又由$w,b$唯一确定，因此有\n",
    "\n",
    "$$ L(f) = L \\left( w,b \\right) $$\n",
    "\n",
    "由于这里所指的损失是指估计值与真实值之间的误差，因此可以用估测误差的平方和来进行总误差的具体估算，即\n",
    "\n",
    "$$ L(f) = \\sum_{n=1}^{10}{\\left( \\hat{y}^n - \\left(  b + wx_{cp}^{n}  \\right) \\right)^2} $$\n",
    "\n",
    "** Step 3: Best function **\n",
    "\n",
    "如何筛选出一个相对于其他model更有优势的model？直观上，如果某个model在拟合给定数据时，它的损失值最小。那么它就是一个理论上的最好的model，即\n",
    "\n",
    "$$ f^* = \\arg \\min_{w, b} L(f) $$\n",
    "\n",
    "或者写为如下形式：\n",
    "\n",
    "$$ f^* = \\arg \\min_{w, b} \\sum_{n=0}^{10}{\\left(  \\hat{y}^{n} - \\left(  b + wx_{cp}^{n}  \\right) \\right)^2}  $$\n",
    "\n",
    "求解该式子的一个办法是，使用Gradient Descent,尽管它不是唯一的一种求解方法，但由于该方法的通用性（适合于任何对$L$可微分的函数），因此被广泛使用。Gradient Descent的常规操作(只考虑其中一个参数$w$)如下：\n",
    "\n",
    "a. 更新参数($w^*$),即令$$ w^* = \\arg \\min _w {L\\left( w \\right)}  $$\n",
    "\n",
    "b. 为$w$初始化一个值$w^0$.\n",
    "\n",
    "c. 计算导数$\\frac{\\mathrm{d} L}{\\mathrm{d} w}|_{w = w^0}$\n",
    "   (一般来说，求得的导数值为negative，则增加$w$的值；而如果导数值为positive,则减小$w$的值)\n",
    "\n",
    "d. 更新参数，即$$ w^1 \\leftarrow w^0 - \\eta \\frac{\\mathrm{d}L}{\\mathrm{d}w}|_{w=w^0}  $$  \n",
    "其中这里的$\\eta$为学习率。\n",
    "\n",
    "e. 当Time step为2时，在已经更新了参数$w$的基础上计算Loss。即令$w=w^1$,再次计算$\\frac{\\mathrm{d} L}{\\mathrm{d} w}|_{w = w^1}$\n",
    "\n",
    "f. 更新参数，即$ w^2 \\leftarrow w^1 - \\eta \\frac{\\mathrm{d}L}{\\mathrm{d}w}|_{w=w^1}  $ \n",
    "\n",
    "g. 以此类推，直至达到loss的local minima / global minima / saddle point / plateau.\n",
    "\n",
    "    \n",
    "![gradient descent](./ml_png/gradient descent.png)\n",
    "\n",
    "Gradient Descent在考虑两个参数$w,b$)时，则每次待更新的参数如下：\n",
    "\n",
    "a. 目标函数中的参数($w^*, b^*$)需更新,即令$$ w^*, b^* = \\arg \\min _{w,b} {L\\left( w, b \\right)}  $$\n",
    "\n",
    "b. 为$w$初始化一个值$w^0$, 计算偏微分$\\frac{\\partial L}{\\partial w}|_{w = w^0}$；为$b$初始化一个值$b^0$, 计算偏微分$\\frac{\\partial L}{\\partial b}|_{b = b^0}$，这两个参数的微分组成了vector形式为：\n",
    "\n",
    "$$\\left[  \n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial L}{\\partial w}|_{w = w^0} \\\\\n",
    "\\frac{\\partial L}{\\partial b}|_{b = b^0}\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "c. 更新参数，即$$ w^1 \\leftarrow w^0 - \\eta \\frac{\\partial L}{\\partial w}|_{w=w^0}  $$ \n",
    "    $$ b^1 \\leftarrow b^0 - \\eta \\frac{\\partial L}{\\partial b}|_{b=b^0}  $$ \n",
    "   它的vector形式为：\n",
    "\n",
    "$$\n",
    "\\left[  \n",
    "\\begin{array}{c}\n",
    "w^1 \\\\\n",
    "b^1\n",
    "\\end{array}\n",
    "\\right]  \\leftarrow\n",
    "\\left[  \n",
    "\\begin{array}{c}\n",
    "w^0 \\\\\n",
    "b^0\n",
    "\\end{array}\n",
    "\\right]   - \\eta\n",
    "\\left[  \n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial  L}{\\partial  w}|_{w = w^0} \\\\\n",
    "\\frac{\\partial  L}{\\partial  b}|_{b = b^0}\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "    依次类推，直接达到终止条件。\n",
    "    \n",
    "![gradient descent2](./ml_png/gradient descent2.png)\n",
    "\n",
    "以上过程的$L$其实是已知的，即$ L(f) = \\sum_{n=1}^{10}{\\left( \\hat{y}^n - \\left(  b + wx_{cp}^{n}  \\right) \\right)^2} $，用该loss函数替换偏微分中的$L$,则有：\n",
    "\n",
    "$$\\frac{\\partial L} {\\partial w} = \\sum_{n=1}^{10}{2 \\left( \\hat{y}^n - \\left(  b + w \\ x_{cp}^{n}  \\right) \\right) \\left( -x_{cp}^{n} \\right)}$$\n",
    "\n",
    "$$\\frac{\\partial L} {\\partial b} = \\sum_{n=1}^{10}{2 \\left( \\hat{y}^n - \\left(  b + w \\ x_{cp}^{n}  \\right) \\right) \\left( -1 \\right)}$$\n",
    "    \n",
    "进一步地，将上述的算法过程结合实际的宝可梦数据进行分析（即计算出实际的$w, b$），结果如下\n",
    "\n",
    "\n",
    "![1](./ml_png/result1.png)\n",
    "\n",
    "可见：\n",
    "\n",
    "（1）Training error和Testing error依然较大（尽管我们的关注点更应该放在Testing error上），此时，自然就要问道，应当如何降低两个error呢？有两个方法，其一是在step1中构造一个更复杂的function，其二是在step2中针对搜集的data做更深入的挖掘。\n",
    "\n",
    "首先，上文提到的step1使用的是线性模型$y = b+w \\ x$，那么如果使用nonlinear model会不会好点，拟合的精度会不会更高？\n",
    "\n",
    "下面考虑几种nonLinear function,第一种是：$$ y = b+w_1 \\ x_{cp} +  w_2 \\ \\left( x_{cp} \\right)^2 $$\n",
    "\n",
    "注意这里的$\\left( x_{cp} \\right)^2 $与$\\ x_{cp}^2 $的不同之处，该模型对应的拟合曲线如下。\n",
    "\n",
    "![2](./ml_png/result2.png)\n",
    "\n",
    "第二种：$$ y = b+w_1 \\ x_{cp} +  w_2 \\ \\left( x_{cp} \\right)^2 +  w_3 \\ \\left( x_{cp} \\right)^3 $$该模型对应的拟合曲线如下。\n",
    "\n",
    "![3](./ml_png/result3.png)\n",
    "\n",
    "\n",
    "第三种：$$ y = b+w_1 \\ x_{cp} +  w_2 \\ \\left( x_{cp} \\right)^2 +  w_3 \\ \\left( x_{cp} \\right)^3 +  w_4 \\ \\left( x_{cp} \\right)^4 $$该模型对应的拟合曲线如下。\n",
    "\n",
    "![4](./ml_png/result4.png)\n",
    "\n",
    "\n",
    "第四种：$$ y = b+w_1 \\ x_{cp} +  w_2 \\ \\left( x_{cp} \\right)^2 +  w_3 \\ \\left( x_{cp} \\right)^3 +  w_4 \\ \\left( x_{cp} \\right)^4  +  w_5 \\ \\left( x_{cp} \\right)^5 $$该模型对应的拟合曲线如下。\n",
    "\n",
    "![5](./ml_png/result5.png)\n",
    "\n",
    "通过以上几种模型，将它们的training error和testing error综合到一张表格，如下：\n",
    "\n",
    "![6](./ml_png/comparision.png)\n",
    "\n",
    "该表格项我们传递了以下几个信息：\n",
    "1）模型复杂度越高，对training data的拟合程度越高，training error越小。\n",
    "\n",
    "2）模型复杂度越高，testing error不见得就会一直降低，这是让模型过度拟合训练数据而不顾testing data的数据噪声、分布规律的情况。结果就是，非线性高到一定程度，error越来越大，该情况常用overfitting这个词来形容。\n",
    "\n",
    "3）结合training error和testing error的曲线走向，找到一个相对较好的模型是可行的。\n",
    "\n",
    "方法二，在收集到更多数据并对这些数据绘制点状图后，发现数据的分布有“扎堆”现象发生，即它的内部可能还包括了多个类别。由于我们前面仅考虑了进化前的cp值，而收集到的宝可梦的species属性（以及其他更多属性）尚未使用到，因此将它们纳入考虑也是顺理成章的。\n",
    "\n",
    "![7](./ml_png/more_species.png)\n",
    "\n",
    "Step 1:\n",
    "\n",
    "$$ if \\  x_s = Pidgey: \\  y = b_1 + w_1 · x_{cp} $$\n",
    "$$ if \\  x_s = Weedle: \\  y = b_2 + w_2 · x_{cp} $$\n",
    "$$ if \\  x_s = Caterpie: \\  y = b_3 + w_3 · x_{cp} $$\n",
    "$$ if \\  x_s = Eevee: \\  y = b_4 + w_4 · x_{cp} $$\n",
    "\n",
    "把它们统一到一个function中去，则有：\n",
    "\n",
    "$$ \\begin{align} \n",
    "  y &= b_1·\\delta \\left( x_s = \\mathrm{Pidgey} \\right) + w_1 ·\\delta \\left( x_s = \\mathrm{Pidgey} \\right) · x_{cp} + \\\\\n",
    "      & \\ \\  \\   b_2·\\delta \\left( x_s = \\mathrm{Weedle} \\right) + w_2 ·\\delta \\left( x_s = \\mathrm{Weedle} \\right) · x_{cp} + \\\\\n",
    "      & \\ \\  \\    b_3·\\delta \\left( x_s = \\mathrm{Caterpie} \\right) + w_3 ·\\delta \\left( x_s = \\mathrm{Caterpie} \\right) · x_{cp} +\\\\\n",
    "      & \\ \\  \\    b_4·\\delta \\left( x_s = \\mathrm{Eevee} \\right) + w_4 ·\\delta \\left( x_s = \\mathrm{Eevee} \\right) · x_{cp} \\\\\n",
    "  \\end{align}   \n",
    "$$ \n",
    "\n",
    "若  \n",
    "$$ \n",
    "  \\delta \\left(  x_s = \\mathrm{Pidgey}  \\right) = \n",
    "  \\left\\{ \\begin{array}{lr} \n",
    "  1, & \\mathrm{If} 𝑥_𝑠=\\mathrm{\"Pidgey\"} \\\\ \n",
    "  0, & \\mathrm{otherwise} \\\\\n",
    "  \\end{array} \\right.  \n",
    "$$\n",
    "则仍有$ y = b_1 + w_1 · x_{cp} $\n",
    "考虑了species后的结果如下\n",
    "\n",
    "![8](./ml_png/more_species_result.png)\n",
    "\n",
    "进一步地，还有其他特征，那么如果把它们也加入进来呢？\n",
    "\n",
    "![9](./ml_png/more_features.png)\n",
    "\n",
    "还是回到Step1中，有\n",
    "\n",
    "$$ if \\  x_s = Pidgey: \\  y' = b_1 + w_1 · x_{cp} + w_5 · \\left( x_{cp} \\right)^2 $$\n",
    "\n",
    "$$ if \\  x_s = Weedle: \\  y' = b_2 + w_2 · x_{cp} + w_6 · \\left( x_{cp} \\right)^2  $$\n",
    "\n",
    "$$ if \\  x_s = Caterpie: \\  y' = b_3 + w_3 · x_{cp} + w_7 · \\left( x_{cp} \\right)^2  $$\n",
    "\n",
    "$$ if \\  x_s = Eevee: \\  y' = b_4 + w_4 · x_{cp} + w_8 · \\left( x_{cp} \\right)^2  $$\n",
    "\n",
    "$$ y = y' + w_9 · x_{hp} + w_{10} · \\left( x_{hp} \\right)^2 + w_{11} · x_{h} + w_{12} · \\left( x_{h} \\right)^2 + w_{13} · x_{w} + w_{14} · \\left( x_{w} \\right)^2  $$\n",
    "\n",
    "结果为，training error=1.9, testing error=102.3,当然该过程还是overfitting的结果。因为testing error过大。\n",
    "\n",
    "解决办法之一是，在Step2中添加regularization term,对weight权值$w_i$进行约束。即，loss function变为如下形式，\n",
    "\n",
    "$$  \n",
    "y = b + \\sum w_i x_{i} \\\\\n",
    "L(w,b) = \\sum_{n}{\\left( \\hat{y}^n - \\left(  b + \\sum w_i x_{i}  \\right) \\right)^2 + \\lambda \\sum{\\left( w_i \\right)^2}} \n",
    "$$\n",
    "\n",
    "$\\lambda \\sum{\\left( w_i \\right)^2}$这一项即为正则项，其中$\\lambda$为自定义的参数。该项存在的意义在于，使函数整体变得更平滑。举例说明，当输入的$x_i$变化了$\\Delta x_i$,则有\n",
    "\n",
    "$$\n",
    "b + \\sum w_i \\left( x_{i} + \\Delta x_i \\right) = y + \\sum w_i \\Delta x_i \n",
    "$$\n",
    "\n",
    "于是，\n",
    "\n",
    "\\left( y + \\sum w_i \\Delta x_i  \\right) - \\left( y \\right) = \\sum w_i \\Delta x_i\n",
    "\n",
    "若其中的每个$w_i$都很小，即便$\\Delta x_i$的值较大，那么也有总的$\\sum w_i \\Delta x_i $较小，也即自变量对因变量的影响不大。尤其是在输入包含了噪声数据的情况下，加入了这个先验项能保证输出结果不受太大影响。另一方面，高阶模型不总是最好的模型，那么通过约束$w_i$的值，也能起到使整体模型不至过复杂的作用，因为低阶模型是对高阶模型的简化。\n",
    "\n",
    "最后，再观察使用了正则化约束项的结果，如图\n",
    "\n",
    "![10](./ml_png/reg.png)\n",
    "\n",
    "上图结果说明了几点，第一，$\\lambda$越大，最终的$w_i$的值越小，Step 1中的function越平滑。这一点在直觉上也能理解。考虑Loss function，我们的目标是要将其最小化，先不考虑前面的data term的情况下，如果$\\lambda$很大，那么为了让总的loss变小，那么只能不断地迫使$w_i$的sum值变小。\n",
    "\n",
    "第二，$\\lambda$值不是越大越好。该值越来越大，代表$w_i$的sum值越来越小，内部的每个$w_i$越来越小，那么就会出现从简化高阶到最后连一个特征/属性也不纳入考虑的一个程度。因此，最理想的情况是，多选择几组$\\lambda$的值，通过实验的方式来发现每个模型的最合适的model值。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y + \\sum w_i \\Delta x_i = b + \\sum w_i \\left( x_{i} + \\Delta x_i \\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3_420]",
   "language": "python",
   "name": "conda-env-Anaconda3_420-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
