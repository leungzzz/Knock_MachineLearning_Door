{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification: Logistic Regression\n",
    "\n",
    "Step 1: Function set\n",
    "\n",
    "上节课推导了概率模型$p_{w,b}(C_1 | x) = \\sigma(z), \\sigma(z) = \\frac{1}{1+\\exp (-z)}$,其中$z = w ·x+b=\\sum_i{w_i x_i + b}$\n",
    "\n",
    "![](./ml_png/sigmoidFunction.png)\n",
    "\n",
    "得到了概率模型后，进行目标的分类，其中\n",
    "\n",
    "$$\\begin{cases}\n",
    "p_{w,b}(C_1 | x) \\geq 0.5, & class \\ 1 \\\\\n",
    "p_{w,b}(C_1 | x) \\lt  0.5 , & class \\ 2 \\\\\n",
    "\\end{cases}  \\tag{1.0}$$\n",
    "\n",
    "结合sigmoid function的函数特性，亦即\n",
    "\n",
    "$$\\begin{cases}\n",
    "z \\geq 0, & class \\ 1 \\\\\n",
    "z \\lt  0 , & class \\ 2 \\\\\n",
    "\\end{cases}  \\tag{1.1}$$\n",
    "\n",
    "针对上面的$z = w ·x+b=\\sum_i{w_i x_i + b}$,可以绘制出下面的图形，\n",
    "\n",
    "![](./ml_png/3_function_set1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Goodness of a Function [Loss function]\n",
    "\n",
    "Training Data: \n",
    "\n",
    "![](./ml_png/3_training_data0.png)\n",
    "\n",
    "这些训练数据是怎么得来的呢？假设它们是根据$f_{w,b}(x) = p_{w,b}(C_1 |x) = p_{\\mu,\\Sigma}(C_1 |x)$这个概率模型采样得来。但是$w,b$的选择是任意的，即不同的概率模型$(\\mu,\\Sigma)$下都有可能获得这些训练数据，区别只在于同时获得这些数据的可能性大小。如果要将这种可能性最大化，用数学公式来表达就是，\n",
    "\n",
    "$$  \n",
    "L(w,b) = f_{w,b}\\left( x^1 \\right) · f_{w,b}\\left( x^2 \\right) · \\left(1-f_{w,b}\\left( x^3 \\right)\\right) · \\dots ·  f_{w,b}\\left( x^N \\right) \n",
    " \\tag{1.2}$$\n",
    "\n",
    "进一步地采用maximum likehood来进行最大化，求出$w^*, b^*$即可，于是有\n",
    "\n",
    "$$ \\begin{align}\n",
    "w^*, b^* &= \\arg \\max_{w,b} L(w,b) \\\\\n",
    "&= \\arg \\min_{w,b} \\Bigl \\{ -\\ln L(w,b) \\Bigr\\}\n",
    "\\end{align}  \\tag{1.3} $$\n",
    "\n",
    "对$-\\ln L(w,b) $, 有\n",
    "\n",
    "$$\\begin{align}\n",
    "-\\ln L(w,b) = &-\\ln f_{w,b}\\left( x^1 \\right)  \\\\\n",
    "&- \\ln f_{w,b}\\left( x^2 \\right)    \\\\\n",
    "&- \\ln \\left(1-f_{w,b}\\left( x^3 \\right)\\right) \\\\\n",
    "&- \\dots  \\\\\n",
    "&- \\ln f_{w,b}\\left( x^N \\right) \\label{123} \\tag{1.4} \n",
    "\\end{align}$$\n",
    "\n",
    "将class 1和class 2的值分别置为1和0（现在为二分类），训练数据输出值为具体的数值\n",
    "\n",
    "![](./ml_png/3_training_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了对公式$(1.4)$进行简化，变成累加的形式，结合$y^n$的具体取值，我们可以得到\n",
    "\n",
    "$$\\begin{align}\n",
    "-\\ln L(w,b) &= \\left[ \\hat{y}^1\\ln f\\left( x^1 \\right) + \\left( 1- \\hat{y}^1 \\right) \\ln \\left(1-f\\left( x^1 \\right) \\right) \\right]  \\\\\n",
    "& \\ \\ \\ -\\left[ \\hat{y}^2\\ln f\\left( x^2 \\right) + \\left( 1- \\hat{y}^2 \\right) \\ln \\left(1-f\\left( x^2 \\right) \\right) \\right]  \\\\\n",
    "& \\ \\ \\ -\\left[ \\hat{y}^3\\ln f\\left( x^3 \\right) + \\left( 1- \\hat{y}^3 \\right) \\ln \\left(1-f\\left( x^3 \\right) \\right) \\right]  \\\\\n",
    "& \\ \\ \\ - \\dots  \\\\\n",
    "& \\ \\ \\ -\\left[ \\hat{y}^N\\ln f\\left( x^N \\right) + \\left( 1- \\hat{y}^N \\right) \\ln \\left(1-f\\left( x^N \\right) \\right) \\right]  \\\\\n",
    "&= \\sum_{n=1}^N{-\\left[ \\hat{y}^n\\ln f\\left( x^n \\right) + \\left( 1- \\hat{y}^n \\right) \\ln \\left(1-f\\left( x^n \\right) \\right) \\right]}\n",
    "\\label{15} \\tag{1.5} \n",
    "\\end{align}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有Bernoulli Distribution p:\n",
    "\n",
    "\\begin{cases}\n",
    "\\mathrm{p} (x=1) = \\hat{y}^n \\\\\n",
    "\\mathrm{p} (x=0) = 1-\\hat{y}^n\n",
    "\\end{cases}\n",
    "\n",
    "同时有Bernoulli Distribution q:\n",
    "\n",
    "\\begin{cases}\n",
    "\\mathrm{q} (x=1) = f(x^n) \\\\\n",
    "\\mathrm{q} (x=0) = 1-f(x^n)\n",
    "\\end{cases}\n",
    "\n",
    "那么它们之间的交叉熵（cross entropy）为：\n",
    "\n",
    "$$\n",
    "H(\\mathrm{p},\\mathrm{q})=  -\\sum_x \\mathrm{p}(x) \\ln\\left( \\mathrm{q}(x) \\right)\n",
    "$$\n",
    "\n",
    "采用one-hot的形式来对物体的真实类别进行管理，则相当于training data符合Bernoulli分布，那么可以认为**公式$(1.5)$也是两个Bernoulli分布之间的cross entropy.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Find the best function \n",
    "\n",
    "目前已经完成了对模型的构造$f_{w,b} = \\mathrm{p}(C_1 | x)$，同时也已经定义损失函数$-\\ln L(w,b)$，接下来要考虑的是对参数的更新，以找到最佳的映射函数。参数更新的常用方法是Gradient Descent，因此先对参数求微分，即\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\Bigl\\{-\\ln L(w,b)\\Bigr\\}}{\\partial w_i} \n",
    "&= \\frac{\\partial \\sum_{n=1}^N{-\\left[ \\hat{y}^n\\ln f\\left( x^n \\right) + \\left( 1- \\hat{y}^n \\right) \\ln \\left(1-f\\left( x^n \\right) \\right) \\right]} }{\\partial w_i}  \\\\\n",
    "&= \\sum_{n=1}^N{-\\left[ \\hat{y}^n  \\frac{\\partial  \\ln f\\left( x^n \\right)}{\\partial w_i}  + \\left( 1- \\hat{y}^n \\right) \\frac{\\partial  \\ln \\left(1-f\\left( x^n \\right) \\right)}{\\partial w_i} \\right]}  \\\\\n",
    "\\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
