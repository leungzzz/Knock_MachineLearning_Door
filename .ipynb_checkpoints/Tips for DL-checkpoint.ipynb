{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for DL\n",
    "\n",
    "Deep learning的常规流程\n",
    "\n",
    "![](./ml_png/6_normal_dl_process.png)\n",
    "\n",
    "一些对应的改进措施：\n",
    "\n",
    "①，在Training data上表现差两个应对方案：\n",
    "\n",
    "    A. New Activation Function\n",
    "    B. Adaptive learning rate\n",
    "    \n",
    "②，在Testing data上表现出的三个应对方案：\n",
    "\n",
    "    A. Early stopping\n",
    "    B. Regularization\n",
    "    c. Dropout\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New activation function\n",
    "\n",
    "问题：为什么要用新的？原来的sigmoid activation function出了什么问题？有哪些新的activation function?\n",
    "\n",
    "回答：随着层数的不断加深，旧的sigmoid function 会引起Vanishing gradient problem.考虑梯度值\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial z}{\\partial w_i}· \\frac{\\partial L}{\\partial z} = \n",
    " \\frac{\\partial z}{\\partial w_i} \\left(  \\frac{\\partial a}{\\partial z}· \\frac{\\partial L}{\\partial a}  \\right) = \n",
    " x_i · \\sigma'(z)· \\left[  w_4 ·  \\frac{\\partial L}{\\partial z'}  +  w_5· \\frac{\\partial L}{\\partial z''}  \\right]\n",
    "$$\n",
    "\n",
    "其中$\\sigma'(z) = \\sigma(z) \\left(  1-\\sigma(z)  \\right), \\ \\ 0<\\sigma(z) < 1$. 如果$z'$和$z''$后面还有上百层的$z^{i}$,那么整个由多个$\\sigma$构成的数值将会变得非常小，如$0.99^{100} \\approx 0.3660$, 当基底变小，有$0.9^{100} \\approx 2.656\\times 10^{-5}$.此时整个gradient会变得非常小。进行gradient descent时，有\n",
    " \n",
    "$$\n",
    "w_1 = w_0 - \\eta \\frac{\\partial L}{\\partial w_0}\n",
    "$$\n",
    "\n",
    "其中$w_0$为初始的随机化weight参数。在反向传播时，越靠近output的gradient由于相乘的$\\sigma$较少，故$\\frac{\\partial L}{\\partial w_N} $较大，学习较快，收敛的速度也会更快。而随着backward pass的不断进行，越靠近Input的gradient,如$w_1$越小，参数学习的速度也相对更慢，可能而$w_1$还在缓慢地向极值靠近，$w_N$就已经根据前面的$w$值收敛到某个极值附近。考虑到所有weight参数都是随机初始的，前面的weight值随机且更新很慢，而后面的weight值已经收敛。极端情况下，靠近input的weight值在随机初始化的初始值附近缓慢更新，而靠近output的weight已经停止更新。此时总的参数更新已经相当缓慢，收敛到某loss值附近，会被误认为已经到达极值附近并发出停止training的错误信号.\n",
    "\n",
    "正是由于这种梯度缓慢消失的情况出现，因此sigmoid被认为不合理，矛盾点在于激活函数的多次相乘所得到的值趋向于零。\n",
    "\n",
    "另一种说法，为什么sigmoid function有问题的解释是，如下图。\n",
    "\n",
    "![](./ml_png/6_relu_reason.png)\n",
    "\n",
    "求偏微分的过程可以简单地理解为自变量的变化（$\\Delta w$）对因变量值的变化（$\\Delta l$）大小，直觉上我们期望这个值大些。因为它反映了这个model的灵敏程度。比如将model用于猫狗分类时，如果loss对输入的dog,cat图片的响应相差不大，那么一定程度上代表这个model很难对不同的class进行分类。而应用sigmoid function时，正是这种情况，即loss对weight的变化不敏感。从上右图sigmoid function两组input值和两组Ouput值可反应出该问题。尤其是当layer不断加深而这种效应的不断叠加，loss前和loss后的值将相差不大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "为了缓解梯度消失的问题，第一个解决方案是activation function更改为ReLU。\n",
    "\n",
    "![](./ml_png/6_relu.png)\n",
    "\n",
    "ReLU的特性。在输出的$z = \\sigma(w_i x_i + b) <0$ 时，$a=0$; 而$z>0$时，$a=z, a'=1$.此时，从$\\frac{\\partial L}{\\partial w_i}$公式的角度便不再出现多个$a'=\\sigma'(z)$的相乘会导致gradient消失的情况。因为$a$值不再在$0<a<1$的范围。\n",
    "\n",
    "如果从Neural Network Structure的角度来看。则可能存在如下可能性。\n",
    "\n",
    "![](./ml_png/6_relu2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果Layer1中的$z_1^3,z_1^4<0$，则$a_1^3,a_1^4=0$,相当于这个neuron被抛弃，同理Layer2中$z$值小于零时有同样的效果。最终去除了$z_1^3,z_1^4,z_2^1, z_2^3$所在的neurons的NN如上图右图示。该过程与overfitting过程的dropout类似（dropout需要定义dropout rate，这里不需要，因此灵活度更大）。分析最终形成的thinner linear network，其中的neurons大幅度减少，总体参数量减少。靠近input的weight的smaller gradients的情况得到缓解，因为全连接权重被压缩为原来的一半，计算gradient需要累乘的$\\sigma'(a)$个数减少为原来的一半。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU的变体有以下两种，它们都是针对$z<0$的情况而言，如下图\n",
    "\n",
    "![](./ml_png/6_leakyrelu_prelu.png)\n",
    "\n",
    "其中Leaky ReLU在$a$轴左侧有斜率$\\alpha=0.01$，parametric ReLU中左侧的斜率值$\\alpha$是学习得到的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了ReLU以外，还有一种激活函数也是常用的,即Maxout。该激活函数的动态的，被称为learnable activation function.这里的maxout就是输出包含了多个$z$的one group中的最大值。如下图\n",
    "\n",
    "![](./ml_png/6_maxout.png)\n",
    "\n",
    "这里的一个neuron包含了$z=\\sum_i w_i x_i +b$和$a=max\\{ z_i^1,z_i^2 \\}$两个变量的求值。Maxout可视为ReLU的更一般形式，或者说，ReLU是Maxout的特殊形式。如下图\n",
    "\n",
    "![](./ml_png/6_maxout_relu.png)\n",
    "\n",
    "左侧是对$z=w x +b$进行ReLU运算，可画出左下角的折线图，其中$z$小于零的地方置零；右侧是比较出直线$z_1=w x +b$和直线$z_2=0 \\times x +0 \\times 1$之间的最大值$a= \\max{z_1, z_2}$，然后取他们的最大值作为输出。需注意的是，Maxout还可以表示除ReLU以外的其他激活函数。分别用$w',b'$来取代上面右侧图的$0,0$,则分别有直线$z_1 = wx+b$和$z_2=w'x+b'$，如下图\n",
    "\n",
    "![](./ml_png/6_maxout_morethan_relu.png)\n",
    "\n",
    "总结：\n",
    "\n",
    "1. 在Maxout Network中，该Maxout function可以是任意的分段线性凸函数；\n",
    "\n",
    "2. “分段”中的段数取决于一组中有多少个$z$值。上面的几幅图中$z$值为2，所有activation function被分为了两段。除此以外，还有可能分三段及以上，如下图所示\n",
    "\n",
    "    ![](./ml_png/6_maxout_group.png)\n",
    "\n",
    "3. Maxout的训练过程分为三步，第一步是先找到每个neuron的最大值；第二步是去除非最大值所在的neuron；第三步是训练剩余的network,如下图。\n",
    "\n",
    "    ![](./ml_png/6_maxout_eventually2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive Learning Rate\n",
    "\n",
    "最开始的参数更新策略为随机梯度下降算法SGD，具体的参数更新流程如下\n",
    "\n",
    "1. 计算$\\nabla L(\\theta^0)$, 更新参数$$\\theta^1 = \\theta^0 - \\eta \\nabla L(\\theta^0)$$\n",
    "\n",
    "2. 计算$\\nabla L(\\theta^1)$, 更新参数$$\\theta^2 = \\theta^1 - \\eta \\nabla L(\\theta^1)$$\n",
    "\n",
    "3. 计算$\\nabla L(\\theta^2)$, 更新参数$$\\theta^3 = \\theta^2 - \\eta \\nabla L(\\theta^2)$$\n",
    "\n",
    "    $\\vdots$\n",
    "\n",
    "4. 计算$\\nabla L(\\theta^{t-1})$, 更新参数$$\\theta^t = \\theta^{t-1} - \\eta \\nabla L(\\theta^{t-1})$$\n",
    "\n",
    "提问：SGD在参数更新过程中有什么缺点，有哪些做法可以弥补之？\n",
    "\n",
    "回答：在loss较平缓的地方，gradient趋于零。理想凸函数中，gradient接近零的地方被为是grobal minima.但实际上loss可能凹凸不平、坑坑洼洼，即gradient接近于零的地方可能是高原 /saddle point / local minima等。而在物理世界中，当我们让一个小球从一个滑坡处滚下，当gradient$\\approx 0$时，它并不会停下来，而是会继续往先前的运动方向继续滑动一段距离。把这个现象引入到参数的更新过程中去，于是就是SGDM,这里的M指的是动量Momentum。\n",
    "\n",
    "##### Momentum\n",
    "\n",
    "Momentum的计算过程如下\n",
    "\n",
    "1. $v^0 = 0$;\n",
    "\n",
    "2. 计算$\\theta^1 = \\theta^0 + v^1$, 其中$$v^1 = \\lambda v^0 - \\eta \\nabla L(\\theta^0)$$\n",
    "\n",
    "3. 计算$\\theta^2 = \\theta^1 + v^2$, 其中\n",
    "$$\\begin{align}\n",
    "v^2 &= \\lambda v^1 - \\eta \\nabla L(\\theta^1)   \\\\\n",
    "&= \\left( \\lambda^2 v^0 - \\lambda^1 \\eta g^0  \\right) - \\lambda^0 \\eta  g^1\n",
    "\\end{align}$$\n",
    "\n",
    "3. 计算$\\theta^3 = \\theta^2 + v^3$, 其中\n",
    "$$\\begin{align}\n",
    "v^3 &= \\lambda v^2 - \\eta \\nabla L(\\theta^2)  \\\\\n",
    "&= \\left( \\lambda^3 v^0 - \\lambda^2 \\eta g^0 - \\lambda^1 \\eta g^1 \\right) - \\lambda^0 \\eta  g^2\n",
    "\\end{align}$$\n",
    "\n",
    "    $\\vdots$\n",
    "\n",
    "4. 计算$\\theta^t = \\theta^{t-1} + v^t$, 其中\n",
    "\n",
    "$$\\begin{align}\n",
    "v^t &= \\lambda v^{t-1} - \\eta \\nabla L(\\theta^{t-1})  \\\\ \n",
    "&= \\left( \\lambda^t v^0 - \\lambda^{t-1} \\eta g^0 - \\lambda^{t-2} \\eta g^1 - \\lambda^{t-3} \\eta g^2 - \\dots - \\lambda^1 \\eta g^{t-2}\\right) - \\lambda^0 \\eta g^{t-1}\n",
    "\\end{align}$$\n",
    "\n",
    "可见，在更新参数值时，不仅仅考虑梯度的方向，还考虑了过去梯度方向汇总的总方向，其中后者我们视其为动量，或者物理环境中的惯性。\n",
    "\n",
    "![](./ml_png/6_momentum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adagrad\n",
    "\n",
    "在实际的NN训练过程中，我们总是希望learning rate是随着Epoch的增大而不断变小，以保证loss稳定地落在在minima而不发生强烈的振荡。而上述的SGD和SGDM其learning rate都是固定不变的$\\eta$，因此，需要找到一个方法使lr能进行自适应地改变。第一种是Adagrad方法，它的数学表示如下\n",
    "\n",
    "$$\n",
    "w^{t+1} \\leftarrow w^t - \\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}{(g^i)}^2}}g^t\n",
    "$$\n",
    "\n",
    "物理意义：在NN中，参数的更新与否通常不是固定的、同步的。在一个Epoch中，可能参数$w_1$被更新，而参数$w_2$不被更新，亦即某些参数在总的Epoch中，可能被更新得频繁些，某些参数的更新频率低些。那么，对于那些被频繁更新的参数，我们在它的身上下了较多的功夫，对它进行了较多的研究并积累了有关的参数选择的丰富知识，这些知识都是通过大量的训练数据习得的。当再次输入一个新样本用于训练时，并不希望该单样本对这个weight参数有很大的影响，即希望该weight的更新速率能慢一些。反之对于那些不被经常更新的参数希望它一旦被更新，其更新速率稍快些。\n",
    "\n",
    "在这里，更新速率的快与慢主要受学习率影响。上式中，$\\sqrt{\\sum_{i=0}^{t}{(g^i)}^2}$是对过去gradient的值的累加。它单调递增，故整体的lr是越来越小的。另一方面，weight被更新得越频繁，那么它的weight值被累加的次数越多，lr越小。反之越大。\n",
    "\n",
    "#### RMSProp / AdaDelta\n",
    "\n",
    "成也萧何败也萧何，随着$\\sqrt{\\sum_{i=0}^{t}{(g^i)}^2}$越来越大，lr将逐渐趋近于零。但如果后续仍有新的training data,那么由于lr过小，NN很难去拟合这些新的data，相当于这些辛苦收集得来的data被白白浪费，甚至影响后面的test精度。归根结底，是因为Adagrad将过去所有的gradient都考虑进来了。如果只将过去某一个时间段内的gradient纳入考虑，是否能解决这个问题？这就是AdaDelta致力于解决的问题，这里的Delta就是指考虑的过去的gradient的比例。它的具体算法如下。\n",
    "\n",
    "$$\\boxed{\\begin{array}{rl}\n",
    "计算g^0，w^1 \\leftarrow w^0 - \\frac{\\eta}{\\delta^0}{g^0}  , & \\delta^0 = g^0 \\\\\n",
    "计算g^1，w^2 \\leftarrow w^1 - \\frac{\\eta}{\\delta^1}{g^1}  , & \\delta^1 = \\sqrt{\\alpha \\left( \\delta^{0} \\right)^2 + \\left( 1-\\alpha\\right) \\left( g^{1} \\right)^2} \\\\\n",
    "计算g^2，w^3 \\leftarrow w^2 - \\frac{\\eta}{\\delta^2}{g^2}  , & \\delta^2 = \\sqrt{\\alpha \\left( \\delta^{1} \\right)^2 + \\left( 1-\\alpha\\right) \\left( g^{2} \\right)^2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "计算g^t，w^{t+1} \\leftarrow w^t - \\frac{\\eta}{\\delta^{t}}{g^t} , & \\delta^t = \\sqrt{\\alpha \\left( \\delta^{t-1} \\right)^2 + \\left( 1-\\alpha\\right) \\left( g^{t} \\right)^2} \\\\\n",
    "\\end{array}}$$\n",
    "\n",
    "#### Adam\n",
    "\n",
    "Adam将同时将动量Momentum和Adaptive learning rate考虑进来，是AdaDelta+Momentum的组合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3_420]",
   "language": "python",
   "name": "conda-env-Anaconda3_420-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
