{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprojection\n",
    "\n",
    "首先，需要知道BP是干嘛的。BP是快速计算gradient的一种方法，在DNN中gradient主要是指对$\\frac {\\partial L}{\\partial W}$的计算。\n",
    "\n",
    "在gradient descent中，每次都需要对DNN中的参数$W,b$的梯度进行更新，其中网络参数有:\n",
    "\n",
    "$$\n",
    "\\theta = \\Bigl\\{  W_1, W_2, \\dots, b_1, b_2, \\dots,\\Bigr\\}\n",
    "$$\n",
    "\n",
    "各网络参数相对于Loss的梯度为\n",
    "\n",
    "$$\n",
    "\\nabla L(\\theta) = \\left[ \\begin{array}{r} \\frac {\\partial L(\\theta)}{\\partial W_1}  \\\\\n",
    "\\frac {\\partial L(\\theta)}{\\partial W_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac {\\partial L(\\theta)}{\\partial b_1}  \\\\\n",
    "\\frac {\\partial L(\\theta)}{\\partial b_2}  \\\\\n",
    "\\vdots \\\\\n",
    "\\end{array}  \\right]\n",
    "$$\n",
    "\n",
    "由于网络参数的迭代过程为：\n",
    "\n",
    "$$\n",
    "\\theta^0  \\longrightarrow \\theta^1 \\longrightarrow \\theta^2 \\longrightarrow \\dots \\longrightarrow \\theta^N\n",
    "$$\n",
    "\n",
    "则每一次的参数更新值可表示为：\n",
    "\n",
    "$$\n",
    "\\mathrm{Compute}  \\  \\nabla L(\\theta^0), \\ \\theta^1 = \\theta^0 - \\eta \\nabla L(\\theta^0) \\\\\n",
    "\\mathrm{Compute}  \\  \\nabla L(\\theta^1), \\ \\theta^2 = \\theta^1 - \\eta \\nabla L(\\theta^1) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathrm{Compute}  \\  \\nabla L(\\theta^{N-1}), \\ \\theta^{N} = \\theta^{N-1} - \\eta \\nabla L(\\theta^{N-1}) \\\\\n",
    "$$\n",
    "\n",
    "在计算$\\frac{\\partial L} {\\partial w_1}$和$\\frac{\\partial L} {\\partial w_2}$时，如果$W_1$和$W_2$是从输入$x_1,x_2$到$Z_1=\\sum_i W_i x_i+b$的weight。那么在计算两个不同的gradient时会出现很多重复的计算。为了高效计算梯度，减少不必要的计算量，backprojection应运而生。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先需要了解的是两种不同形式的chain rule.\n",
    "\n",
    "<mark style=background-color:yellow>形式一（直链式）</mark>\n",
    "\n",
    "两个不同的函数用公式表示为\n",
    "\n",
    "$$\n",
    "y = g(x), z=h(y)\n",
    "$$\n",
    "\n",
    "图表达形式为\n",
    "\n",
    "$$\n",
    "\\Delta x  \\rightarrow \\Delta y \\rightarrow \\Delta z\n",
    "$$\n",
    "\n",
    "求函数$z$相对于$x$的微分。有\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} z}{\\mathrm{d}x} = \\frac{\\mathrm{d} y}{\\mathrm{d}x} · \\frac{\\mathrm{d} z}{\\mathrm{d}y} \n",
    "$$\n",
    "\n",
    "\n",
    "<mark style=background-color:yellow>形式二（分叉式）</mark>\n",
    "\n",
    "函数表示为\n",
    "\n",
    "$$\n",
    "x = g(s), y=h(s),z=k(x,y)\n",
    "$$\n",
    "\n",
    "图表达形式为\n",
    "\n",
    "$$\\begin{matrix}\n",
    "& & \\Delta x & &  \\\\\n",
    "& \\nearrow & & \\searrow &  \\\\\n",
    "\\Delta s & & & & \\Delta z  \\\\\n",
    "& \\searrow & & \\nearrow  &  \\\\\n",
    "& & \\Delta y  & &  \\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "求函数$z$相对于$s$的微分。有\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} z}{\\mathrm{d}s} = \\frac{\\mathrm{d} x}{\\mathrm{d}s} · \\frac{\\mathrm{d} z}{\\mathrm{d}x} + \n",
    "\\frac{\\mathrm{d} y}{\\mathrm{d}s} · \\frac{\\mathrm{d} z}{\\mathrm{d}y} \n",
    "$$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
